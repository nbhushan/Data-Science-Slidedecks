---
sort: 6
---

# Linear Algebra and Linear models

In today's workshop, you will learn a tiny bit about the deep relationship that
exists between machine learning and linear algrabra. In particular, we will focus on
understanding how concepts from linear algebra can be used to obtain
linear regression models.

## Today's learning objectives
- [ ] Understand the relationship between linear algebrea and linear models.
- [ ] Familiarise yourself with the normal equations to solve linear models.

### Linear Regression - a Linear Algebra perspective

Recall that while building a linear regression models, we generally have an
outcome variable **y**, which we want to predict using a set of predictor
variables **X**. For example, **y** could refer to the number of ice creams
sold, and **X** could refer to the temperature.

In linear algebra notation, **y** is then a column vector of variables.

**Y** = [[y<sub>1</sub>, y<sub>2</sub>, y<sub>3</sub>, ..., y<sub>N</sub>]]

And in the case of a simple linear regression, **x** is a column vector similar
in shape to **y**.

**x** = [[x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>N</sub>]]

In the case of multiple linear regression, **X** is now a matrix of predictor
variables. Assuming we have N rows of K variables:

**X** = [[x<sub>1,1</sub>, x<sub>1,2</sub>, x<sub>1,3</sub>, ..., x<sub>1,K</sub>],
          [x<sub>2,1</sub>, x<sub>2,2</sub>, x<sub>2,3</sub>, ..., x<sub>2,K</sub>],
          ...,
          [x<sub>N,1</sub>, x<sub>N,2</sub>, x<sub>N,3</sub>, ..., x<sub>N,K</sub>]]


We cam now write down our linear regression model as

**y** = ```beta```.**X**

Where: <br>
``beta`` is a vector of regression coefficients <br>
``error`` is the irreducible error

Recall from the previous lesson that linear systems can be represented in linear
algebra form as:

**Ax = b**

Since we have now represented our linear regression model as a linear system,
we can solve it using tools of linear algebra.

### Solving linear regression - the Normal equations

The solution to the linear regression problem we stated as

**y** = ```beta```.**X**

is given by

 ```beta_hat``` = (**X<sup>T</sup>X**)<sup>-1.</sup>**X<sup>T</sup>y**

 > Proving the normal equations requires mathematical tools that will be
introduced in Year 2. We will re-visit the normal equations then :).

>Today's takeaway is that the normal equations are beautiful and a direct application of
 linear algebra concepts to machine learning!

# Preparation for the DataLab

- [ ] Please complete with the ```Linear Algebra``` course in Code Academy which can
 be found [here](https://www.codecademy.com/learn/learn-linear-algebra) and
 **upload a certificate of completion** to your learning logs.
